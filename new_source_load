    source_load_df = spark.read \
        .options(**options) \
        .text(
        f"{os.getenv('STAGING_INPUT_PATH')}/{job_arguments.batch_date}/{job_arguments.source_name}/{job_arguments.file_name}/{job_arguments.file_segment_name}*")

options = {
            "header": "true",
            "inferSchema": "false",
            "delimiter": delimiter_mapper.get(file_parameters.file_format)
}

STAGING_INPUT_PATH=/mnt/edp-work-ewr-mount/ewr/staging/unprocessed

    delimiter_mapper = {
        "psv": "|",
        "csv": ","
    }


(select plan_id,substring(collect_set(role_key)[0],1,3) as company_code from dss_dc_producerrole_daily_dat group by plan_id) as dss on coalesce(dss.plan_id, '-9999') = coalesce(pld.plan_number, '-9999')
